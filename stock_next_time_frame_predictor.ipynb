{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc3c8ff7",
   "metadata": {},
   "source": [
    "## Build Functions for ELT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30a2670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Requirments (Updated on 10/2/2024, streamlit 1.39.0)\n",
    "# !pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b487bd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from yfinance import Ticker\n",
    "from pykalman import KalmanFilter\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "import pprint\n",
    "\n",
    "from src import modules as f # f is for function\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61147f2",
   "metadata": {},
   "source": [
    "### Download, Transform, and Modeling All in One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c0ce4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manufactoring suppliers \n",
    "# 'lrcs', 'klac', ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bebab82",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$AAPL: possibly delisted; no price data found  (5m 2024-09-26 -> 2024-11-20 07:00:11-05:00)\n",
      "$AAPL: possibly delisted; no price data found  (5m 2024-09-26 -> 2024-11-20 07:00:11-05:00)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1,1) and (0,) not aligned: 1 (dim 1) != 0 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maapl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/stock_production/src/modules.py:550\u001b[0m, in \u001b[0;36mpredictions\u001b[0;34m(symbol)\u001b[0m\n\u001b[1;32m    540\u001b[0m intervals \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5m\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    541\u001b[0m              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m15m\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    542\u001b[0m              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1h\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    545\u001b[0m              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1mo\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    546\u001b[0m             ]\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m interval \u001b[38;5;129;01min\u001b[39;00m intervals:\n\u001b[0;32m--> 550\u001b[0m     time_stamp, summary_table \u001b[38;5;241m=\u001b[39m \u001b[43mdl_tf_pd\u001b[49m\u001b[43m(\u001b[49m\u001b[43msymbol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_dl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# skip_dl=True on redo's\u001b[39;00m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msymbol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minterval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Interval Timestamp: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_stamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    552\u001b[0m     col_headers \u001b[38;5;241m=\u001b[39m summary_table\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m~/Documents/github/stock_production/src/modules.py:528\u001b[0m, in \u001b[0;36mdl_tf_pd\u001b[0;34m(symbol, interval, skip_dl)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_dl:\n\u001b[1;32m    527\u001b[0m     download(symbol, interval)\n\u001b[0;32m--> 528\u001b[0m     \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msymbol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    529\u001b[0m     curr_prediction, models, feature_names, classification_reports \u001b[38;5;241m=\u001b[39m model(symbol, interval)\n\u001b[1;32m    530\u001b[0m     predictions, prediction_probas \u001b[38;5;241m=\u001b[39m make_prediction(models, curr_prediction, feature_names)\n",
      "File \u001b[0;32m~/Documents/github/stock_production/src/modules.py:179\u001b[0m, in \u001b[0;36mtransform\u001b[0;34m(symbol, interval)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Kalman filtering (noise reduction algorithm) \u001b[39;00m\n\u001b[1;32m    171\u001b[0m kf \u001b[38;5;241m=\u001b[39m KalmanFilter(transition_matrices \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    172\u001b[0m                   observation_matrices \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    173\u001b[0m                   initial_state_mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    176\u001b[0m                   transition_covariance\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m    177\u001b[0m                  )\n\u001b[0;32m--> 179\u001b[0m state_means, _ \u001b[38;5;241m=\u001b[39m \u001b[43mkf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madj_close\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m state_means \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(state_means\u001b[38;5;241m.\u001b[39mflatten(), index\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m    181\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkma\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m state_means\n",
      "File \u001b[0;32m~/Documents/github/stock_production/pyenv/lib/python3.12/site-packages/pykalman/standard.py:1170\u001b[0m, in \u001b[0;36mKalmanFilter.filter\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1160\u001b[0m Z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_observations(X)\n\u001b[1;32m   1162\u001b[0m (transition_matrices, transition_offsets, transition_covariance,\n\u001b[1;32m   1163\u001b[0m  observation_matrices, observation_offsets, observation_covariance,\n\u001b[1;32m   1164\u001b[0m  initial_state_mean, initial_state_covariance) \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_parameters()\n\u001b[1;32m   1166\u001b[0m )\n\u001b[1;32m   1168\u001b[0m (_, _, _, filtered_state_means,\n\u001b[1;32m   1169\u001b[0m  filtered_state_covariances) \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1170\u001b[0m     \u001b[43m_filter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransition_matrices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation_matrices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransition_covariance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation_covariance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransition_offsets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation_offsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitial_state_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_state_covariance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mZ\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m )\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (filtered_state_means, filtered_state_covariances)\n",
      "File \u001b[0;32m~/Documents/github/stock_production/pyenv/lib/python3.12/site-packages/pykalman/standard.py:388\u001b[0m, in \u001b[0;36m_filter\u001b[0;34m(transition_matrices, observation_matrices, transition_covariance, observation_covariance, transition_offsets, observation_offsets, initial_state_mean, initial_state_covariance, observations)\u001b[0m\n\u001b[1;32m    384\u001b[0m     observation_covariance \u001b[38;5;241m=\u001b[39m _last_dims(observation_covariance, t)\n\u001b[1;32m    385\u001b[0m     observation_offset \u001b[38;5;241m=\u001b[39m _last_dims(observation_offsets, t, ndims\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    386\u001b[0m     (kalman_gains[t], filtered_state_means[t],\n\u001b[1;32m    387\u001b[0m      filtered_state_covariances[t]) \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 388\u001b[0m         \u001b[43m_filter_correct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m            \u001b[49m\u001b[43mobservation_covariance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m            \u001b[49m\u001b[43mobservation_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredicted_state_means\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredicted_state_covariances\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m            \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m     )\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (predicted_state_means, predicted_state_covariances,\n\u001b[1;32m    398\u001b[0m         kalman_gains, filtered_state_means,\n\u001b[1;32m    399\u001b[0m         filtered_state_covariances)\n",
      "File \u001b[0;32m~/Documents/github/stock_production/pyenv/lib/python3.12/site-packages/pykalman/standard.py:278\u001b[0m, in \u001b[0;36m_filter_correct\u001b[0;34m(observation_matrix, observation_covariance, observation_offset, predicted_state_mean, predicted_state_covariance, observation)\u001b[0m\n\u001b[1;32m    263\u001b[0m     predicted_observation_covariance \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    264\u001b[0m         np\u001b[38;5;241m.\u001b[39mdot(observation_matrix,\n\u001b[1;32m    265\u001b[0m                np\u001b[38;5;241m.\u001b[39mdot(predicted_state_covariance,\n\u001b[1;32m    266\u001b[0m                       observation_matrix\u001b[38;5;241m.\u001b[39mT))\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;241m+\u001b[39m observation_covariance\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    270\u001b[0m     kalman_gain \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    271\u001b[0m         np\u001b[38;5;241m.\u001b[39mdot(predicted_state_covariance,\n\u001b[1;32m    272\u001b[0m                np\u001b[38;5;241m.\u001b[39mdot(observation_matrix\u001b[38;5;241m.\u001b[39mT,\n\u001b[1;32m    273\u001b[0m                       linalg\u001b[38;5;241m.\u001b[39mpinv(predicted_observation_covariance)))\n\u001b[1;32m    274\u001b[0m     )\n\u001b[1;32m    276\u001b[0m     corrected_state_mean \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    277\u001b[0m         predicted_state_mean\n\u001b[0;32m--> 278\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkalman_gain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpredicted_observation_mean\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     )\n\u001b[1;32m    280\u001b[0m     corrected_state_covariance \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    281\u001b[0m         predicted_state_covariance\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(kalman_gain,\n\u001b[1;32m    283\u001b[0m                  np\u001b[38;5;241m.\u001b[39mdot(observation_matrix,\n\u001b[1;32m    284\u001b[0m                         predicted_state_covariance))\n\u001b[1;32m    285\u001b[0m     )\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1,1) and (0,) not aligned: 1 (dim 1) != 0 (dim 0)"
     ]
    }
   ],
   "source": [
    "f.predictions('aapl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecd7cca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e774a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faffd6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b681aeb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af29e9f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee10080b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9032138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73486d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44b90ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c246e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bfe9c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac48269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a74260a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22751626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebb6752",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "symbol='NVDA'\n",
    "interval='1d'\n",
    "\n",
    "# # Define Eastern Time Zone\n",
    "# eastern = pytz.timezone('US/Eastern')\n",
    "\n",
    "# # Get current time in Eastern Time Zone\n",
    "# eastern_time = datetime.now(eastern)\n",
    "\n",
    "# # Format the time to include hour, minute, and seconds\n",
    "# time_stamp = eastern_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# print(f'DL Time: {time_stamp}')\n",
    "\n",
    "# f.download(symbol, interval, period)\n",
    "f.transform(symbol, interval)\n",
    "curr_prediction, models, feature_names, classification_reports = f.model(symbol, interval)\n",
    "predictions, prediction_probas = f.make_prediction(models, curr_prediction, feature_names)\n",
    "\n",
    "f.predictions_summary(predictions, prediction_probas, classification_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5d5ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6050b670",
   "metadata": {},
   "outputs": [],
   "source": [
    "models['XGBoost'].feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3373277",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905656b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_list = sorted(zip(feature_names, models['XGBoost'].feature_importances_),\n",
    "                     key=lambda x: x[1], \n",
    "                     reverse=True\n",
    "                    )\n",
    "merged_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2f74ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a2c40f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "classification_reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1fb54a",
   "metadata": {},
   "source": [
    "### Hyperparameter Searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a303ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def model(symbol, interval, search_type='none'):\n",
    "    # Load data\n",
    "    data = f.load_model_df(symbol, interval)\n",
    "    data.dropna(inplace=True, axis=0)\n",
    "    X = data.drop(columns=['direction'], axis=1)\n",
    "    y = data['direction']\n",
    "    \n",
    "    # Print column names to check for issues\n",
    "    print(\"Columns in X before preprocessing:\")\n",
    "    print(X.columns)\n",
    "    \n",
    "    # Remove duplicate columns\n",
    "    X = X.loc[:, ~X.columns.duplicated()]\n",
    "    \n",
    "    # Check if categorical_features are present in X\n",
    "    categorical_features = ['day_of_month', 'day_of_week', 'hour_of_day']\n",
    "    missing_features = [col for col in categorical_features if col not in X.columns]\n",
    "    if missing_features:\n",
    "        print(f\"Missing categorical features: {missing_features}\")\n",
    "    \n",
    "    # Store current prediction data (last row)\n",
    "    curr_prediction = X.iloc[-1].copy()\n",
    "\n",
    "    # Drop last row from X and y to prevent the model from seeing it\n",
    "    X = X.iloc[:-1]\n",
    "    y = y.iloc[:-1]\n",
    "    \n",
    "    # Create the categorical transformer\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    \n",
    "    # Create the preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        force_int_remainder_cols=False # This will include all other columns in the transformed output\n",
    "    )\n",
    "    \n",
    "    # Define your models\n",
    "    models = {\n",
    "        'XGBoost': XGBClassifier(random_state=42, n_jobs=-1),\n",
    "        'RandomForest': RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "        'GradientBoosting': GradientBoostingClassifier(random_state=42, validation_fraction=0.25, n_iter_no_change=31),\n",
    "        # 'LightGBM': LGBMClassifier(random_state=42,force_col_wise=True),\n",
    "        'KNN': KNeighborsClassifier(n_neighbors=7, p=1,weights='distance')\n",
    "    }\n",
    "    \n",
    "    # Hyperparameters to search\n",
    "    param_grids = {\n",
    "        'XGBoost': {\n",
    "            'classifier__n_estimators': [100, 200, 300],\n",
    "            'classifier__max_depth': [3, 5, 7, 9],\n",
    "            'classifier__learning_rate': [0.01, 0.1, 0.2, 0.3]\n",
    "        },\n",
    "        'RandomForest': {\n",
    "            'classifier__n_estimators': [100, 200, 300],\n",
    "            'classifier__max_depth': [None, 10, 20, 30],\n",
    "            'classifier__min_samples_split': [2, 5, 10, 13]\n",
    "        },\n",
    "        'GradientBoosting': {\n",
    "            'classifier__n_estimators': [100, 200, 300, 400],\n",
    "            'classifier__max_depth': [3, 5, 7, 13],\n",
    "            'classifier__le||arning_rate': [0.01, 0.1, 0.2, 0.5]\n",
    "        },\n",
    "        'KNN': {\n",
    "            'classifier__n_neighbors': [3, 5, 7, 13],\n",
    "            'classifier__weights': ['uniform', 'distance'],\n",
    "            'classifier__p': [1, 2]  # 1: Manhattan, 2: Euclidean\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create a function to get feature names after transformation\n",
    "    def get_feature_names_out(column_transformer):\n",
    "        feature_names = []\n",
    "        for name, transformer, columns in column_transformer.transformers_:\n",
    "            if transformer == 'drop' or transformer == 'passthrough':\n",
    "                if transformer == 'passthrough':\n",
    "                    feature_names.extend(columns)\n",
    "                continue\n",
    "            if hasattr(transformer, 'get_feature_names_out'):\n",
    "                names = transformer.get_feature_names_out(columns)\n",
    "                feature_names.extend(names)\n",
    "            else:\n",
    "                feature_names.extend(columns)\n",
    "        return feature_names\n",
    "    \n",
    "    # Split data before preprocessing to avoid data leakage\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Fit the preprocessor on training data\n",
    "    preprocessor.fit(X_train)\n",
    "    \n",
    "    # Transform training and test data\n",
    "    X_train_transformed = preprocessor.transform(X_train)\n",
    "    X_test_transformed = preprocessor.transform(X_test)\n",
    "    \n",
    "    # Get feature names after transformation\n",
    "    feature_names = get_feature_names_out(preprocessor)\n",
    "    \n",
    "    # Convert transformed data to DataFrame\n",
    "    X_train_transformed = pd.DataFrame(X_train_transformed, columns=feature_names)\n",
    "    X_test_transformed = pd.DataFrame(X_test_transformed, columns=feature_names)\n",
    "    \n",
    "    # Transform curr_prediction\n",
    "    curr_prediction_transformed = preprocessor.transform(\n",
    "        curr_prediction.to_frame().T)\n",
    "    curr_prediction_transformed = pd.DataFrame(\n",
    "        curr_prediction_transformed, columns=feature_names)\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        # Create a pipeline with the classifier\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "        \n",
    "        # Get the parameter grid for the current model\n",
    "        param_grid = param_grids.get(model_name, {})\n",
    "        \n",
    "        # Use GridSearchCV or RandomizedSearchCV\n",
    "        if search_type == 'grid' and param_grid:\n",
    "            search = GridSearchCV(\n",
    "                pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "        elif search_type == 'random' and param_grid:\n",
    "            search = RandomizedSearchCV(\n",
    "                pipeline, param_grid, cv=5, scoring='accuracy',\n",
    "                n_jobs=-1, n_iter=10, random_state=42)\n",
    "        else:\n",
    "            search = pipeline\n",
    "        \n",
    "        # Fit the model\n",
    "        search.fit(X_train_transformed, y_train)\n",
    "        \n",
    "        # If using search, get the best estimator\n",
    "        if search_type in ['grid', 'random'] and param_grid:\n",
    "            best_model = search.best_estimator_\n",
    "            print(f\"Best parameters for {model_name}: {search.best_params_}\")\n",
    "            model = best_model.named_steps['classifier']\n",
    "        else:\n",
    "            model = search.named_steps['classifier']\n",
    "        \n",
    "        # Store the model\n",
    "        models[model_name] = model\n",
    "        \n",
    "        # Predict on test data\n",
    "        y_pred = search.predict(X_test_transformed)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(classification_report(y_test, y_pred, zero_division=0))\n",
    "    \n",
    "    return curr_prediction_transformed, models, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c358417",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "curr_prediction, models, feature_names = model('AMD', '5m', 'grid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11f7294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d379dcfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "pyenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
